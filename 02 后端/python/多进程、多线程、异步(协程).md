多进程：适合CPU密集型任务
多线程：由于GIL全局解释器锁(使用CPython解释器)的原因，多线程都是伪多线程，适合IO密集型
协程：异步IO，适合IO密集型，拥有比多线程更低的县城切换开销和锁开销，同时协程的控制代码非常简单，都在一个线程内，也不需要锁这些东西。
> 通常来讲，使用多进程+协程即可，可以同时应对CPU密集型任务和IO密集型任务

## 多进程常用操作
实现多进程有多种方式：
1.`multiprocessing`
创建一个进程来做指定任务
```python 
import os
from multiprocessing import Process

def worker():
    print(f"Worker process ID: {os.getpid()}")

if __name__ == "__main__":
    print(f"Main process ID: {os.getpid()}")

    # 创建一个进程对象
    p = Process(target=worker)

    # 启动进程
    p.start()

    # 等待子进程结束
    p.join()

```
2.`Pool` 进程池
创建多个任务推荐进程池，可以避免反复创建进程
```python
import time
from multiprocessing import Pool

def worker(x):
    time.sleep(1)  # 模拟耗时操作
    return x * x

if __name__ == "__main__":
    with Pool(processes=4) as pool:
        result = pool.apply(worker, (5,))
        print(result)

```
如果参数为一个list，可以直接使用 `pool.map` 来创建，更方便，除此之外还有几个进程池的函数都有不同的效果。
```python
import time
from multiprocessing import Pool

def worker(x):
    time.sleep(1)  # 模拟耗时操作
    return x * x

if __name__ == "__main__":
    with Pool(processes=4) as pool:
        results = pool.map(worker, range(10))
        print(results)

```
同时线程池是不需要 `start` 函数来启动的，结束则使用 `pool.join()` 函数则可以等待进程池全部结束后运行其它代码
进程之间通信，需要使用 Queue：
```python
from multiprocessing import Process, Queue

def sender(q, data):
    q.put(data)

def receiver(q):
    while True:
        data = q.get()
        if data is None:
            break
        print(f"Received: {data}")

if __name__ == "__main__":
    q = Queue()

    # 创建发送者进程
    send_process = Process(target=sender, args=(q, "Hello!"))
    send_process.start()
    send_process.join()

    # 创建接收者进程
    receive_process = Process(target=receiver, args=(q,))
    receive_process.start()
    q.put(None)  # 发送结束信号
    receive_process.join()

```
在进程之间进行数据共享，可以使用Manager，其实就是类似前端的bus：
```python
from multiprocessing import Manager, Process

def worker(shared_list, value):
    shared_list.append(value)
    print(f"Process {value}: {shared_list}")

if __name__ == "__main__":
    manager = Manager()
    shared_list = manager.list()

    processes = []
    for i in range(5):
        p = Process(target=worker, args=(shared_list, i))
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    print("Final shared list:", shared_list)

```
## 多线程常用操作
通常使用 `threading` 实现，这里暂不研究，使用协程是更好的选择。
## 协程(异步操作)
```python
import asyncio

async def count():
    print("One")
    await asyncio.sleep(1)
    print("Two")

async def main():
    await asyncio.gather(count(), count(), count())

asyncio.run(main())
```
上面运行结果的原因是，三个 `count()` 依次执行，打印完 `One`，就休眠1秒钟，把执行权交给下一个 `count()`，所以先连续打印出三个 `One`。等到1秒钟休眠结束，执行权重新交回第一个 `count()`，开始执行 `await` 命令下一行的语句，所以会接着打印出三个`Two`。脚本总的运行时间是1秒。
需要明确的是，`await` 并不是线程就停止在这里了，而是去执行其它异步函数，只是控制等到异步完毕再执行 `await` 后的代码而已，当然如果只执行一个异步函数，那使用 `await` 等于让这个异步函数变为了同步函数。

