## 理论相关
全开源和半开源：全开源是将训练数据、训练过程、训练后的权重model全部开源，而半开源只有训练后的权重model，目前大多数都是半开源，全开源的有：Pythia、OLMo
## 模型相关
权重值Weights：权重是神经网络中的参数，它们连接着网络中的神经元。每个权重表示一个神经元对另一个神经元影响的强度。在一个神经网络中，每个连接都有与之对应的权重值，这些权重值在训练过程中通过学习算法进行调整，以便网络能够更准确地执行特定任务。简单来说就是参数。
激活值：激活值是指神经元在接收到输入信号后，经过激活函数处理后的输出值。在神经网络中，每一层的每个神经元的输出都是一个激活值，这些激活值随后会作为下一层的输入。简单来说就是输出的值。
7B、70B：B为10亿，表示model有70亿、700亿权重值(参数)，每10亿参数在float32(4字节)情况下大约占用3.75GB，通常我们估算1B大约占用4GB，7B模型也就需要28G，int8那就只需要7G，int4更是只需要3B左右。
## 量化
量化：默认model采用float32，也就是32位byte的浮点数来表示每一个参数，7B则需要70亿 x 4字节(32位就是4字节) =  27.2 GB，这对于普通电脑来说太大了，所有需要量化，比如将每个参数使用int8表示，也就是8位来表示一个参数，那么7B则只需要70亿 x 1字节 = 6.8 GB，总的来说量化就是将每个参数使用4位或8位代替原本的32位来表示。
量化深入：注意32使用float表示，4和8使用的是int表示，指的是将32位浮点数转化为8位的整数。那么如果参数含有小数位就直接丢失了，比如有个参数表示人的身高183.8，那经过转换就直接丢失了。另外也不要考虑将float32转换为float8，float8首先就不是标准的float格式，就算符合标准，也不科学，本身就只有8位只能表示256个数，还用其中的1位或者多位来表示小数，那这个参数的范围大小就大大缩小了，比如一个重量参数，总共就能表示256，还去掉几位，大小范围被大大缩小了。（注意：前面只是对参数举例不是正确的，只是假设，大模型的参数不代表现实世界的身高、重量，而是权重参数，权重参数是模型用来乘以输入特征以产生输出结果的数值。它们决定了每个输入特征对模型输出的影响程度）
GPTQ：使用更复杂的量化方法和策略来对模型量化，精度损失更小，[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)，已经集成到Transformers中了。
GGML：使用GGML量化，模型名称除了带有GGML外，还有q4、q4_0、q5等，需要使用这些模型时再研究。
GGUF：GGML推出的新功能，可以在模型中添加额外的信息，现在一般都是使用GGUF。
GPTQ 和 GGML 是现在模型量化的两种主要方式，两者有以下几点异同：
- GPTQ 在 **GPU** 上运行较快，而 GGML 在 **CPU** 上运行较快
- 同等精度的量化模型，GGML(GGUF) 的模型要比 GPTQ 的稍微大一些，但是两者的推理性能基本一致
- 两者都可以量化 HuggingFace 上的 Transformer 模型
## 代码相关
tokenizer：mapping映射，将token转为具体的id，或者将id转为具体的token。
llama.cpp、chatglm.cpp、xxx.cpp：这种都是各种开源模型使用C++重写了Python代码，可以提升效率，并且支持使用CPU来运行，源于llama.cpp，现在大家都基本会这么做一个自己框架的cpp应用。
deepspeed：这个框架的主要作用是将模型分布在不同显卡、cpu上训练、微调。








