## 什么是fine-tuning
给pre-trained的model添加一些capability，而不是外挂knowledge，这些capability是包含在model内部的，但是需要注意，增加了外部的能力，极有可能原有的能力会下降。
![[CleanShot 2024-08-15 at 11.11.47.png]]
这里要区分一下从零开始训练模型和微调模型对于数据的要求，通常微调模型对数据要求特别低，只需要输入和输出组成的数据对即可，因为pre-trained llm已经具备理解的能力，而训练一个pre-trained模型则需要更多的原始数据、标注和特征，两者对数据的要求完全不同。
微调支持同时加强多种类型的能力。
## 什么时候需要Fine-tuning
首先来看一下大模型项目pipeline：
![[CleanShot 2024-08-15 at 10.33.24.png]]
如果通过Prompt engineering可以解决问题，那就不要进行Fine-tuning，比如采用Few-shot或者Cot等方法
![[CleanShot 2024-08-15 at 11.00.33.png]]
但采用Few-shot存在的问题是：
1.例子太多，比如task需要判断类别，类别的样本包含20种，那few-shot至少需要20个。
2.即使传递了大于20的样本，有可能效果还是不佳。
如果在遇到上面的问题时，就可以考虑使用fine-tuning，但优先考虑一定还是使用few-shot或者cot。
## Fine-tune流程
![[CleanShot 2024-08-15 at 14.51.45.png]]
### 数据构造
数据均需要成prompt的形式<input,output>
数据集包含训练集、验证集和测试集，验证集在训练的时候验证，测试集用于测试微调好的model
验证就相当于一个学生平时的各种模拟考试，而测试则是最终的高考，测试集在训练时不可见。
### 模型遗忘
微调之后，大模型原本的能力丢失，比如微调增加情感分析能力，但是原本的正常问题，回答都会加上positive或negative。
解决思路：
1.不解决，反正拥有想要的能力即可
2.使用更大的model，微调的数据微不足道，对原本model的影响就非常小
3.缺啥补啥，哪个能力丢失了，就再微调以下这方面的能力
### 选择模型
微调使用的开源模型选择很多，通常要根据硬件水平、模型能力等来综合选择。
当然可以同时微调多个模型，来综合对比哪个pre-trained model效果更好。
### 分析模型
在选择model的过程中，我们要判断这些model缺乏哪些能力，也就是gap，model的answer和我们目标的差距，了解gap之后，再根据gap来进行数据收集和fine tune，而不是上来就直接进行fine tune。
### 微调方式
1.全量微调：所有参数都调一遍，影响范围大，不推荐使用
2.freeze微调：不推荐使用
3.lora微调：推荐使用
## Lora
![[CleanShot 2024-08-15 at 14.57.11.png]]
通常在使用lora微调时，会将原本的参数冻结，不参与梯度更新。
lora微调之后，需要和原本的参数进行合并。
tokenizer：将token转为具体的id，或者将id转为具体的token
常见lora微调框架：
LLAMA-Factory：国产，支持一键微调各种国内国外的框架，[GitHub - hiyouga/LLaMA-Factory: A WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024)](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)
## 微调参数
先明确一些基本的概念：
- 权重值Weights：权重是神经网络中的参数，它们连接着网络中的神经元。每个权重表示一个神经元对另一个神经元影响的强度。在一个神经网络中，每个连接都有与之对应的权重值，这些权重值在训练过程中通过学习算法进行调整，以便网络能够更准确地执行特定任务。简单来说就是参数。
- 激活值：激活值是指神经元在接收到输入信号后，经过激活函数处理后的输出值。在神经网络中，每一层的每个神经元的输出都是一个激活值，这些激活值随后会作为下一层的输入。简单来说就是输出的值。
- 7B、70B：B为10亿，表示model有70亿、700亿权重值(参数)
- 量化：默认model采用float32，也就是32位byte的浮点数来表示每一个参数，7B则需要70亿 x 4字节(32位就是4字节) =  27.2 GB，这对于普通电脑来说太大了，所有需要量化，比如将每个参数使用int8表示，也就是8位来表示一个参数，那么7B则只需要70亿 x 1字节 = 6.8 GB，总的来说量化就是将每个参数使用4位或8位代替原本的32位来表示。
- 量化深入：注意32使用float表示，4和8使用的是int表示，指的是将32位浮点数转化为8位的整数。那么如果参数含有小数位就直接丢失了，比如有个参数表示人的身高183.8，那经过转换就直接丢失了。另外也不要考虑将float32转换为float8，float8首先就不是标准的float格式，就算符合标准，也不科学，本身就只有8位只能表示256个数，还用其中的1位或者多位来表示小数，那这个参数的范围大小就大大缩小了，比如一个重量参数，总共就能表示256，还去掉几位，大小范围被大大缩小了。（注意：前面只是对参数举例不是正确的，只是假设，大模型的参数不代表现实世界的身高、重量，而是权重参数，权重参数是模型用来乘以输入特征以产生输出结果的数值。它们决定了每个输入特征对模型输出的影响程度）


常用微调参数：
量化等级：int4、int8、fp16、bf16(比fp16精度更高，需要一些硬件支持)



